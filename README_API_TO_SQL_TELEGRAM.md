# API to SQL Telegram Bot

Скрипт для загрузки данных о недвижимости из API Bayut в базу данных PostgreSQL с возможностью управления через Telegram бота.

## Функциональность

- Загрузка только новых данных о недвижимости из API Bayut
- Автоматическое определение и загрузка только объявлений, отсутствующих в базе данных
- Сохранение данных в базу данных PostgreSQL
- Опциональное сохранение данных в CSV файл
- Управление через Telegram бота

## Требования

- Python 3.6+
- Установленные пакеты:
  - requests
  - pandas
  - psycopg2
  - python-dotenv
  - python-telegram-bot

## Настройка

1. Создайте файл `.env` в корневой директории проекта со следующими параметрами:
```env
RAPIDAPI_KEY=ваш_ключ_api
DB_NAME=имя_базы_данных
DB_USER=пользователь
DB_PASSWORD=пароль
DB_HOST=хост
DB_PORT=порт
TELEGRAM_BOT_TOKEN=токен_вашего_бота
EMAIL_ADMIN=адрес_для_отчетов (опционально)
EMAIL_ADMIN_PASSWORD=пароль_для_почты (опционально)
```

2. Создайте бота в Telegram через @BotFather и получите токен

3. Установите необходимые пакеты:
```bash
pip install -r requirements.txt
```

## Использование

### Запуск скрипта загрузки API в SQL

```bash
python api_to_sql.py
```

### Параметры командной строки

- `--limit N` - максимальное количество новых записей для загрузки из API (по умолчанию 1000)
- `--no-csv` - не сохранять данные в CSV файл (только в SQL)
- `--send-email` - отправлять email-отчёт после загрузки

### Примеры использования

1. Загрузить новые объявления с сохранением в CSV (не более 1000):
```bash
python api_to_sql.py
```

2. Загрузить не более 500 новых записей без сохранения в CSV:
```bash
python api_to_sql.py --limit 500 --no-csv
```

3. Загрузить новые записи с отправкой email-уведомления:
```bash
python api_to_sql.py --send-email
```

## Алгоритм загрузки новых объявлений

Скрипт использует умную логику загрузки, которая позволяет отбирать только новые объявления:

1. При запуске скрипт определяет последнюю загруженную дату создания объявления в базе данных
2. Отправляет запросы к API, получая объявления, отсортированные по дате создания (от новых к старым)
3. Последовательно обрабатывает каждую страницу результатов, пока не встретит объявление, уже имеющееся в базе данных
4. После обнаружения первого "старого" объявления загрузка останавливается, так как все последующие уже загружены
5. Загружает только новые объявления в базу данных

Такой подход минимизирует количество запросов к API и обеспечивает загрузку только актуальных данных.

## Команды Telegram бота

- `/start` - начало работы с ботом
- `/help` - показать список доступных команд
- `/load N` - загрузить N записей из API (например: `/load 50`)
- `/status` - показать статус последней загрузки
- `/stats` - показать статистику загрузок

## Структура проекта

```
.
├── api_to_sql.py          # Основной скрипт
├── .env                   # Файл с настройками
├── requirements.txt       # Зависимости проекта
├── logs/                  # Директория для логов
└── Api_Bayat/            # Директория для CSV файлов
```

## Логирование

Скрипт ведет подробное логирование всех операций. Логи сохраняются в директории `logs/` с именем файла в формате:
```
api_to_sql_YYYYMMDD_HHMMSS.log
```

## Обработка ошибок

Скрипт включает обработку следующих ошибок:
- Ошибки подключения к API
- Ошибки подключения к базе данных
- Ошибки при загрузке данных
- Ошибки при сохранении в CSV

## Безопасность

- Все чувствительные данные хранятся в файле `.env`
- Файл `.env` добавлен в `.gitignore`
- Используется безопасное подключение к базе данных
- Проверка валидности входных данных

## Поддержка

При возникновении проблем:
1. Проверьте логи в директории `logs/`
2. Убедитесь, что все настройки в `.env` корректны
3. Проверьте подключение к интернету и доступность API
4. Проверьте доступность базы данных

# Инструкция по созданию таблицы для хранения данных из Bayut

Перед запуском скриптов убедитесь, что в вашей базе данных создана таблица для хранения данных о недвижимости. Используйте следующий SQL-запрос:

```sql
CREATE TABLE IF NOT EXISTS bayut_properties (
    id BIGINT PRIMARY KEY,
    title TEXT,
    price NUMERIC,
    rooms INTEGER,
    baths INTEGER,
    area NUMERIC,
    rent_frequency VARCHAR(50),
    location TEXT,
    cover_photo_url TEXT,
    property_url TEXT,
    category VARCHAR(100),
    property_type VARCHAR(100),
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    furnishing_status VARCHAR(100),
    completion_status VARCHAR(100),
    amenities TEXT,
    agency_name TEXT,
    contact_info TEXT,
    geography TEXT,
    agency_logo_url TEXT,
    proxy_mobile VARCHAR(50),
    keywords JSONB,
    is_verified BOOLEAN,
    purpose VARCHAR(50),
    floor_number INTEGER,
    city_level_score NUMERIC,
    score NUMERIC,
    agency_licenses JSONB,
    agency_rating NUMERIC
);

-- Опциональные индексы для повышения производительности
CREATE INDEX IF NOT EXISTS idx_bayut_properties_created_at ON bayut_properties(created_at);
CREATE INDEX IF NOT EXISTS idx_bayut_properties_updated_at ON bayut_properties(updated_at);
CREATE INDEX IF NOT EXISTS idx_bayut_properties_price ON bayut_properties(price);
CREATE INDEX IF NOT EXISTS idx_bayut_properties_location ON bayut_properties(location);
```

**Пояснения:**
- `id` — уникальный идентификатор объекта, используется как PRIMARY KEY.
- `created_at` — дата и время создания объявления, используется для определения новых объявлений.
- `updated_at` — дата и время последнего обновления объявления.
- Созданы индексы для повышения производительности запросов к базе данных.

---

# Инструкция по загрузке и публикации данных

## 1. Загрузка данных из API в CSV

Для выгрузки данных из API Bayut используется скрипт `api_to_csv.py`.

**По умолчанию** скрипт загружает **4000 последних записей**.

### Как использовать:

- **Загрузить 4000 записей (по умолчанию):**
  ```
  python api_to_csv.py
  ```

- **Загрузить определённое количество записей (например, 10):**
  ```
  python api_to_csv.py --limit 10
  ```
  Параметр `--limit` задаёт, сколько свежих объявлений будет загружено.  
  Если параметр не указан, используется значение по умолчанию — 4000.

- **Дополнительные параметры:**
  - `--since YYYY-MM-DD` — дата начала (опционально)
  - `--until YYYY-MM-DD` — дата окончания (по умолчанию сегодня)
  - `--max-pages N` — максимальное число страниц (обычно не требуется)

**Результат:**  
CSV-файл с данными будет сохранён в папке `Api_Bayat` с именем вида `bayut_properties_sale_YYYYMMDD.csv`.

---

## 2. Загрузка данных из CSV в базу данных

Для загрузки данных из CSV в базу используется скрипт:

```
python csv_to_sql.py Api_Bayat\bayut_properties_sale_YYYYMMDD.csv
```

- Вместо `YYYYMMDD` подставьте актуальную дату файла, который вы хотите загрузить.

---

## 3. Публикация анализа в Telegram

Для публикации результатов в Telegram используйте:

```
python telegram_publisher.py
```

- Скрипт автоматически возьмёт свежие данные из базы, сформирует анализ и опубликует его в ваш Telegram-канал.
- Все параметры доступа (токены, ID канала) должны быть прописаны в файле `.env` в корне проекта.

---

## Важно

- Если хотите загрузить другое количество записей из API — всегда используйте параметр `--limit`.
- Все основные скрипты должны находиться в корневом каталоге проекта.
- Для корректной работы публикации в Telegram убедитесь, что файл `.env` присутствует и содержит актуальные токены.

---

# Механизм расписания публикаций и автоматизации загрузки

В проекте реализован универсальный планировщик публикаций и загрузки данных — `publication_scheduler.py`.

## Как это работает

- Все задачи (загрузка из API, публикации в Telegram и др.) запускаются автоматически по расписанию, заданному в файле `schedule_config.json`.
- Планировщик читает расписание и запускает нужные скрипты в нужное время.
- Вся информация о работе планировщика сохраняется в лог-файл `logs/scheduler.log`.

## Пример расписания (schedule_config.json)
```json
{
    "publications": [
        {
            "script_name": "telegram_publisher.py",
            "days": ["понедельник"],
            "time": "09:00"
        },
        {
            "script_name": "medium_apartments_publisher.py",
            "days": ["вторник"],
            "time": "09:00"
        },
        {
            "script_name": "price_changes_publisher.py",
            "days": ["среда"],
            "time": "09:00"
        },
        {
            "script_name": "find_cheapest_apartments_langchain.py",
            "days": ["четверг"],
            "time": "09:00"
        },
        {
            "script_name": "api_to_sql.py",
            "days": ["понедельник", "вторник", "среда", "четверг", "пятница", "суббота", "воскресенье"],
            "time": "08:00"
        }
    ]
}
```

## Как запустить планировщик

1. Убедитесь, что все скрипты из расписания существуют в корне проекта.
2. Запустите планировщик одной из команд:
   ```
   python publication_scheduler.py
   ```
   или для Windows:
   ```
   start_scheduler.bat
   ```
3. Планировщик будет работать в фоне и автоматически запускать задачи по расписанию.
4. Все события и ошибки пишутся в `logs/scheduler.log`.

## Управление расписанием

- **Посмотреть расписание:**
  Откройте файл `schedule_config.json`.
- **Изменить расписание:**
  Отредактируйте `schedule_config.json` (добавьте/удалите задачи, измените время или дни).
- **Логи:**
  Вся информация о работе — в `logs/scheduler.log`.

## Примечания
- Планировщик использует библиотеку `schedule` (установите через `pip install schedule`).
- Для автозапуска при старте Windows используйте ярлык на `start_scheduler.bat` в папке автозагрузки.
- Все скрипты должны корректно завершаться и выводить информацию в стандартный вывод.

---

**Если возникнут вопросы по настройке или работе шедулера — смотрите логи или обращайтесь к разработчику.**

# Журнал изменений

## Версия от [текущая дата]

### Улучшение загрузки API данных
- **Изменена логика загрузки из API**: теперь скрипт загружает только новые объявления, которые еще не были добавлены в базу данных
- **Оптимизированы запросы к API**: загрузка останавливается, когда встречаются ранее загруженные объявления
- **Улучшена обработка дат**: добавлена корректировка timestamp для более точного определения новых объявлений
- **Обновлены настройки по умолчанию**: значение по умолчанию для параметра --limit увеличено до 1000
- **Улучшена обработка отсутствия новых данных**: скрипт корректно обрабатывает ситуацию, когда новых объявлений нет
- **Улучшено логирование**: добавлен вывод количества загруженных объявлений в реальном времени

### Улучшение производительности базы данных
- **Добавлены индексы** для повышения скорости работы с базой данных
- **Оптимизирован SQL-запрос** для вставки данных, теперь используется ON CONFLICT (id)
- **Улучшено подключение к базе данных** для более надежной работы

### Улучшение уведомлений
- **Добавлена отправка email-отчетов** с результатами загрузки (с параметром --send-email)
- **Улучшены сообщения в логах** для лучшего отслеживания процесса загрузки 