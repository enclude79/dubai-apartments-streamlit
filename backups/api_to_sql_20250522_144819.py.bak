"""
Скрипт для загрузки данных о недвижимости из API Bayut в базу данных PostgreSQL.

Функциональность:
- Загрузка только новых и обновленных данных о недвижимости из API Bayut (отсутствующих в базе данных)
- Сохранение данных в базу данных PostgreSQL
- Опциональное сохранение данных в CSV файл

Алгоритм загрузки:
1. Определение последней даты обновления записи в базе данных (по полю updated_at)
2. Загрузка объявлений из API, начиная с самых новых
3. Останов загрузки при достижении уже сохраненных ранее записей (по дате обновления)
4. Сохранение только новых и обновленных записей в базу данных

Параметры запуска:
--limit N       Максимальное количество записей для загрузки из API (по умолчанию 1000)
--no-csv        Не сохранять данные в CSV файл (только в SQL)
--send-email    Отправлять email-отчёт после загрузки (по умолчанию отключено)

Примеры использования:
1. Загрузить новые и обновленные объявления с сохранением в CSV (не более 1000):
   python api_to_sql.py

2. Загрузить не более 500 новых и обновленных записей без сохранения в CSV:
   python api_to_sql.py --limit 500 --no-csv

3. Загрузить новые и обновленные записи с отправкой email-уведомления:
   python api_to_sql.py --send-email

Требования:
- Python 3.6+
- Установленные пакеты: requests, pandas, psycopg2, python-dotenv
- Файл .env с настройками:
  RAPIDAPI_KEY=ваш_ключ_api
  DB_NAME=имя_базы_данных
  DB_USER=пользователь
  DB_PASSWORD=пароль
  DB_HOST=хост
  DB_PORT=порт
  EMAIL_ADMIN=адрес_для_отчетов (опционально)
  EMAIL_ADMIN_PASSWORD=пароль_для_почты (опционально)
"""

import os
import requests
import json
import csv
import time
import pandas as pd
import psycopg2
import logging
import argparse
import datetime
from datetime import datetime, timedelta
from dotenv import load_dotenv
import smtplib
from email.message import EmailMessage
import numpy as np
import psycopg2.extras  # Добавляем импорт extras для работы с JSON

# Настройка логирования
log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)
log_filename = f'{log_dir}/api_to_sql_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Параметры API
API_CONFIG = {
    "url": "https://bayut.p.rapidapi.com/properties/list",
    "headers": {
        "X-RapidAPI-Key": os.getenv('RAPIDAPI_KEY'),
        "X-RapidAPI-Host": "bayut.p.rapidapi.com"
    },
    "params": {
        "locationExternalIDs": "5002,6020",  # Дубай
        "purpose": "for-sale",
        "hitsPerPage": "25",
        "sort": "date-desc",  # Сортировка по дате (сначала новые)
        "categoryExternalID": "4",  # Квартиры
        "isDeveloper": "true",  # Только от застройщиков
        "completionStatus": ["off-plan", "under-construction"]  # Строящиеся объекты
    }
}

# Пути к файлам
CSV_DIR = "Api_Bayat"
os.makedirs(CSV_DIR, exist_ok=True)

# Загружаем переменные окружения
load_dotenv()

# Проверяем наличие API ключа
if not os.getenv('RAPIDAPI_KEY'):
    logger.error("RAPIDAPI_KEY не найден в переменных окружения")
    print("Ошибка: RAPIDAPI_KEY не найден в переменных окружения")
    print("Пожалуйста, создайте файл .env и добавьте в него:")
    print("RAPIDAPI_KEY=ваш_ключ_api")
    exit(1)
else:
    print(f"API ключ загружен: {os.getenv('RAPIDAPI_KEY')[:10]}...")
    logger.info(f"API ключ загружен: {os.getenv('RAPIDAPI_KEY')[:10]}...")

# Параметры базы данных
DB_CONFIG = {
    'dbname': os.getenv('DB_NAME', 'postgres'),
    'user': os.getenv('DB_USER', 'admin'),
    'password': os.getenv('DB_PASSWORD', 'Enclude79'),
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432'),
    'table': 'bayut_properties'
}

class DatabaseConnection:
    """Класс для работы с подключением к базе данных"""
    
    def __init__(self, db_config):
        self.db_config = db_config
        self.connection = None
    
    def connect(self):
        """Подключается к базе данных"""
        try:
            self.connection = psycopg2.connect(
                dbname=self.db_config['dbname'],
                user=self.db_config['user'],
                password=self.db_config['password'],
                host=self.db_config['host'],
                port=self.db_config['port']
            )
            self.connection.set_client_encoding('UTF8')
            self.connection.autocommit = True
            logger.info("Успешное подключение к базе данных")
            return True
        except Exception as e:
            logger.error(f"Ошибка при подключении к базе данных: {e}")
            return False
    
    def close(self):
        """Закрывает подключение к базе данных"""
        if self.connection:
            self.connection.close()
            logger.info("Подключение к базе данных закрыто")
    
    def execute_query(self, query, params=None):
        """Выполняет SQL запрос"""
        if not self.connection:
            logger.error("Нет активного подключения к базе данных")
            return False
        
        cursor = self.connection.cursor()
        try:
            cursor.execute(query, params)
            logger.info("SQL запрос выполнен успешно")
            return True
        except Exception as e:
            logger.error(f"Ошибка при выполнении SQL запроса: {e}")
            return False
        finally:
            cursor.close()

def update_last_run_info(last_updated_date):
    """Сохраняет информацию о последнем запуске, включая дату последнего обновления"""
    if not last_updated_date:
        logger.error("Невозможно обновить информацию о последнем запуске: нет даты обновления")
        return False
    
    logger.info(f"ОТЛАДКА: Начало обновления last_run_info с датой: {last_updated_date}")
    print(f"ОТЛАДКА: Начало обновления last_run_info с датой: {last_updated_date}")
    
    try:
        conn = psycopg2.connect(
            dbname=DB_CONFIG['dbname'],
            user=DB_CONFIG['user'],
            password=DB_CONFIG['password'],
            host=DB_CONFIG['host'],
            port=DB_CONFIG['port']
        )
        conn.autocommit = True
        cur = conn.cursor()
        
        # Проверяем существование таблицы
        cur.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_name = 'last_run_info'
            );
        """)
        table_exists = cur.fetchone()[0]
        logger.info(f"ОТЛАДКА: Таблица last_run_info существует: {table_exists}")
        print(f"ОТЛАДКА: Таблица last_run_info существует: {table_exists}")
        
        # Создаем таблицу, если не существует
        if not table_exists:
            logger.info("Создание таблицы last_run_info для хранения информации о запусках")
            cur.execute("""
                CREATE TABLE last_run_info (
                    id SERIAL PRIMARY KEY,
                    script_name VARCHAR(255) NOT NULL,
                    last_run TIMESTAMP,
                    last_updated_date TIMESTAMP,
                    status VARCHAR(50),
                    records_processed INTEGER,
                    UNIQUE(script_name)
                );
            """)
            logger.info("ОТЛАДКА: Таблица last_run_info создана")
            print("ОТЛАДКА: Таблица last_run_info создана")
        
        # Обновляем информацию о последнем запуске
        try:
            cur.execute("""
                INSERT INTO last_run_info (script_name, last_run, last_updated_date, status, records_processed)
                VALUES ('api_to_sql', NOW(), %s, 'SUCCESS', 0)
                ON CONFLICT (script_name) DO UPDATE 
                SET last_run = NOW(), 
                    last_updated_date = %s,
                    status = 'SUCCESS';
            """, (last_updated_date, last_updated_date))
            
            logger.info(f"Информация о последнем запуске обновлена: последняя дата обновления = {last_updated_date}")
            print(f"Информация о последнем запуске обновлена: последняя дата обновления = {last_updated_date}")
            
            # Проверяем, успешно ли обновились данные
            cur.execute("SELECT last_updated_date FROM last_run_info WHERE script_name = 'api_to_sql';")
            result = cur.fetchone()
            if result:
                logger.info(f"ОТЛАДКА: Проверка - в last_run_info сохранена дата: {result[0]}")
                print(f"ОТЛАДКА: Проверка - в last_run_info сохранена дата: {result[0]}")
            else:
                logger.warning("ОТЛАДКА: После обновления данные в last_run_info не найдены!")
                print("ОТЛАДКА: После обновления данные в last_run_info не найдены!")
        except Exception as e:
            logger.error(f"ОТЛАДКА: Ошибка при выполнении SQL в update_last_run_info: {e}")
            print(f"ОТЛАДКА: Ошибка при выполнении SQL в update_last_run_info: {e}")
            raise
            
        cur.close()
        conn.close()
        return True
    except Exception as e:
        logger.error(f"Ошибка при обновлении информации о последнем запуске: {e}")
        print(f"Ошибка при обновлении информации о последнем запуске: {e}")
        return False

def get_last_updated_at():
    """Получает максимальное значение updated_at из базы данных или таблицы last_run_info"""
    try:
        conn = psycopg2.connect(
            dbname=DB_CONFIG['dbname'],
            user=DB_CONFIG['user'],
            password=DB_CONFIG['password'],
            host=DB_CONFIG['host'],
            port=DB_CONFIG['port']
        )
        cur = conn.cursor()
        
        # Сначала проверяем таблицу last_run_info
        cur.execute("SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'last_run_info');")
        table_exists = cur.fetchone()[0]
        logger.info(f"ОТЛАДКА: В get_last_updated_at: таблица last_run_info существует: {table_exists}")
        print(f"ОТЛАДКА: В get_last_updated_at: таблица last_run_info существует: {table_exists}")
        
        if table_exists:
            cur.execute("SELECT last_updated_date FROM last_run_info WHERE script_name = 'api_to_sql';")
            result = cur.fetchone()
            if result and result[0]:
                last_updated_date = result[0]
                # Вычитаем 1 секунду, чтобы избежать проблем с точностью timestamp в БД
                adjusted_result = last_updated_date - timedelta(seconds=1)
                logger.info(f"Последняя дата обновления из таблицы last_run_info: {last_updated_date} (скорректированная: {adjusted_result})")
                print(f"ОТЛАДКА: Найдена дата в last_run_info: {last_updated_date} (скорректированная: {adjusted_result})")
                cur.close()
                conn.close()
                return adjusted_result
            else:
                logger.info("ОТЛАДКА: Запись в last_run_info существует, но last_updated_date пуст или отсутствует")
                print("ОТЛАДКА: Запись в last_run_info существует, но last_updated_date пуст или отсутствует")
        
        # Если нет информации в last_run_info, используем максимальную дату из bayut_properties
        cur.execute("SELECT MAX(updated_at) FROM bayut_properties;")
        result = cur.fetchone()[0]
        cur.close()
        conn.close()
        
        if result:
            # Вычитаем 1 секунду, чтобы избежать проблем с точностью timestamp в БД
            adjusted_result = result - timedelta(seconds=1)
            logger.info(f"Последняя дата обновления в базе данных: {result} (скорректированная: {adjusted_result})")
            print(f"ОТЛАДКА: Найдена дата в bayut_properties: {result} (скорректированная: {adjusted_result})")
            return adjusted_result
            
        logger.info("В базе данных нет записей")
        return None  # None если записей нет
    except Exception as e:
        logger.error(f"Ошибка при получении последней даты обновления: {e}")
        print(f"ОТЛАДКА: Ошибка в get_last_updated_at: {e}")
        return None

def fetch_properties(max_pages=None, max_records=1000):
    """Загружает только новые записи через API Bayut, останавливаясь, когда встречаются уже загруженные объявления"""
    logger.info(f"Начало загрузки последних записей через API Bayut (лимит: {max_records})")
    all_properties = []
    last_updated_date = None
    
    # Получаем последнюю дату обновления из базы данных
    last_updated_at = get_last_updated_at()
    if last_updated_at:
        logger.info(f"Будут загружены только объявления, обновленные после {last_updated_at}")
        print(f"Будут загружены только объявления, обновленные после {last_updated_at}")
    else:
        logger.info("В базе данных нет записей, будут загружены все доступные объявления")
        print("В базе данных нет записей, будут загружены все доступные объявления")
    
    # Настройки для оптимизации запросов
    page_size = 25  # Ограничение API по количеству записей на страницу
    
    # Максимальное количество страниц для проверки
    if not max_pages:
        max_pages = (max_records + page_size - 1) // page_size
    
    total_loaded = 0
    found_old_records = False
    for page in range(1, max_pages + 1):
        logger.info(f"Запрос #{page}: получение записей (страница {page})")
        print(f"Запрос #{page}: получение записей (страница {page}) - загружено {total_loaded} новых объявлений")
        
        querystring = API_CONFIG["params"].copy()
        querystring["page"] = str(page)
        querystring["hitsPerPage"] = str(page_size)
        
        try:
            # Делаем запрос
            response = requests.get(
                API_CONFIG["url"],
                headers=API_CONFIG["headers"],
                params=querystring
            )
            
            # Проверяем заголовки rate limit
            rate_limit = response.headers.get('X-RapidAPI-RateLimit-Limit', '')
            rate_remaining = response.headers.get('X-RapidAPI-RateLimit-Remaining', '')
            if rate_limit and rate_remaining:
                logger.info(f"Rate limit: {rate_remaining}/{rate_limit} запросов осталось")
            
            response.raise_for_status()
            data = response.json()
            
            hits = data.get('hits', [])
            if not hits:
                logger.info(f"Данные не найдены в запросе #{page}")
                break
            
            new_records_in_page = 0
            for item in hits:
                # Преобразуем updated_at в datetime объект для сравнения
                updated_at_timestamp = item.get('updatedAt', 0)
                updated_at_datetime = datetime.fromtimestamp(updated_at_timestamp)
                
                # Если есть last_updated_at и текущее объявление обновлено раньше или одновременно с последним в базе,
                # значит, мы уже загрузили это и последующие объявления
                if last_updated_at and updated_at_datetime <= last_updated_at:
                    logger.info(f"Найдено уже загруженное объявление (ID: {item.get('id')}, дата обновления: {updated_at_datetime}, последняя дата в БД: {last_updated_at})")
                    found_old_records = True
                    break
                
                # Обновляем последнюю дату загрузки
                if last_updated_date is None or updated_at_datetime > last_updated_date:
                    last_updated_date = updated_at_datetime
                
                # Добавляем новое объявление в список
                all_properties.append(extract_property_data(item))
                new_records_in_page += 1
                total_loaded += 1
                
                # Если достигли лимита записей, останавливаемся
                if len(all_properties) >= max_records:
                    logger.info(f"Достигнут лимит количества записей: {max_records}")
                    break
            
            logger.info(f"Получено {new_records_in_page} новых объектов в запросе #{page}")
            
            # Если в текущей странице нашли старое объявление, больше не запрашиваем
            if found_old_records:
                logger.info("Найдены уже загруженные объявления, прекращаем загрузку")
                print("Найдены уже загруженные объявления, прекращаем загрузку")
                break
                
            # Если достигли лимита записей, останавливаемся
            if len(all_properties) >= max_records:
                break
                
            # Если на текущей странице нет объявлений, прекращаем загрузку
            if new_records_in_page == 0:
                logger.info("На текущей странице нет новых объявлений, прекращаем загрузку")
                print("На текущей странице нет новых объявлений, прекращаем загрузку")
                break
                
            # Задержка между запросами для соблюдения rate limit
            if page < max_pages:
                time.sleep(5)  # 5 секунд между запросами
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Ошибка при запросе к API (запрос #{page}): {e}")
            if len(all_properties) == 0:
                return [], None  # Если первый запрос не удался, возвращаем пустой список
            break  # Прерываем получение данных, но возвращаем уже полученные
        except Exception as e:
            logger.error(f"Неожиданная ошибка в запросе #{page}: {e}")
            if len(all_properties) == 0:
                return [], None
            break
    
    # Логируем итоги загрузки
    logger.info(f"Загрузка завершена. Всего загружено новых объектов: {len(all_properties)}")
    print(f"Загрузка завершена. Всего загружено новых объектов: {len(all_properties)}")
    
    if last_updated_date:
        logger.info(f"Последняя дата обновления в загруженных данных: {last_updated_date}")
        print(f"Последняя дата обновления в загруженных данных: {last_updated_date}")
    
    return all_properties, last_updated_date

def extract_property_data(property_item):
    """Извлекает данные из объекта API в формат для сохранения"""
    created_at = datetime.fromtimestamp(property_item.get('createdAt', 0))
    updated_at = datetime.fromtimestamp(property_item.get('updatedAt', 0))
    
    # Извлекаем данные из сложной структуры JSON
    location = ""
    if property_item.get('location'):
        locations = property_item.get('location', [])
        # Берем название места с самым высоким уровнем детализации (обычно уровень 2 - район)
        for loc in locations:
            if loc.get('level') == 2 and loc.get('name'):
                location = loc.get('name')
                break
        # Если не найдено, берем любой доступный уровень
        if not location and len(locations) > 0:
            location = locations[-1].get('name', '')
    
    # Получаем список удобств
    amenities_list = []
    for amenity in property_item.get('amenities', []):
        if isinstance(amenity, dict) and amenity.get('text'):
            amenities_list.append(amenity.get('text'))
    amenities_str = ', '.join(amenities_list)
    
    # Формируем контактную информацию
    phone_number = property_item.get('phoneNumber', {})
    contact_info = f"Тел: {phone_number.get('mobile', '?')}; WhatsApp: {phone_number.get('whatsapp', '?')}"
    
    # Формируем географические данные
    geography = property_item.get('geography', {})
    geo_info = f"Широта: {geography.get('lat')}, Долгота: {geography.get('lng')}"
    
    # Категория (берем первую, если есть)
    category_name = ""
    categories = property_item.get('category', [])
    if categories and isinstance(categories, list) and len(categories) > 0:
        if isinstance(categories[0], dict):
            category_name = categories[0].get('name', '')
    
    # Формируем итоговый словарь данных
    return {
        'id': property_item.get('id'),
        'title': property_item.get('title'),
        'price': property_item.get('price'),
        'rooms': property_item.get('rooms'),
        'baths': property_item.get('baths'),
        'area': property_item.get('area'),
        'rent_frequency': property_item.get('rentFrequency'),
        'location': location,
        'cover_photo_url': property_item.get('coverPhoto', {}).get('url'),
        'property_url': f"https://www.bayut.com/property/details-{property_item.get('externalID')}.html",
        'category': category_name,
        'property_type': property_item.get('propertyType'),
        'created_at': created_at.strftime('%Y-%m-%d %H:%M:%S'),
        'updated_at': updated_at.strftime('%Y-%m-%d %H:%M:%S'),
        'furnishing_status': property_item.get('furnishingStatus'),
        'completion_status': property_item.get('completionStatus'),
        'amenities': amenities_str,
        'agency_name': property_item.get('agency', {}).get('name'),
        'contact_info': contact_info,
        'geography': geo_info,
        'agency_logo_url': property_item.get('agency', {}).get('logo', {}).get('url'),
        'proxy_mobile': phone_number.get('proxyMobile'),
        'keywords': json.dumps(property_item.get('keywords', [])),
        'is_verified': property_item.get('isVerified'),
        'purpose': property_item.get('purpose'),
        'floor_number': property_item.get('floorNumber'),
        'city_level_score': property_item.get('cityLevelScore'),
        'score': property_item.get('score'),
        'agency_licenses': json.dumps(property_item.get('agency', {}).get('licenses', [])),
        'agency_rating': property_item.get('agency', {}).get('rating')
    }

def save_to_csv(properties_data):
    """Сохраняет данные о недвижимости в CSV файл"""
    if not properties_data:
        logger.warning("Нет данных для сохранения в CSV")
        print("Нет данных для сохранения в CSV")
        return None
    
    # Проверяем, не пуст ли список данных
    if len(properties_data) == 0:
        logger.warning("Пустой список данных, CSV-файл не будет создан")
        print("Пустой список данных, CSV-файл не будет создан")
        return None
    
    # Формируем имя файла с текущей датой
    current_date = datetime.now().strftime("%Y%m%d")
    output_file = os.path.join(CSV_DIR, f'bayut_properties_sale_{current_date}.csv')
    
    # Определяем заголовки CSV из ключей первого объекта
    fieldnames = list(properties_data[0].keys())
    
    # Сохраняем в CSV с кодировкой UTF-8
    with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
        writer.writeheader()
        writer.writerows(properties_data)
    
    logger.info(f"Данные ({len(properties_data)} записей) сохранены в файл: {output_file}")
    print(f"Данные ({len(properties_data)} записей) сохранены в файл: {output_file}")
    
    return output_file

def clean_text(text):
    """Очищает текст от символов, не поддерживаемых WIN1251 (cp1251)"""
    if not isinstance(text, str):
        return text
    try:
        # Перекодируем в cp1251, заменяя неподдерживаемые символы на '?'
        return text.encode('cp1251', errors='replace').decode('cp1251')
    except Exception:
        return text.encode('ascii', 'ignore').decode('ascii')

def send_email_report(subject, body, to_email):
    """Отправляет email-отчёт администратору о процессе загрузки"""
    smtp_server = 'smtp.yandex.ru'
    smtp_port = 465
    from_email = os.getenv('EMAIL_ADMIN')
    password = os.getenv('EMAIL_ADMIN_PASSWORD')
    if not from_email or not password:
        logger.warning("EMAIL_ADMIN или EMAIL_ADMIN_PASSWORD не заданы, письмо не отправлено")
        return False
    try:
        msg = EmailMessage()
        msg['Subject'] = subject
        msg['From'] = from_email
        msg['To'] = to_email
        msg.set_content(body)
        with smtplib.SMTP_SSL(smtp_server, smtp_port) as server:
            server.login(from_email, password)
            server.send_message(msg)
        logger.info(f"Email-отчёт отправлен на {to_email}")
        return True
    except Exception as e:
        logger.error(f"Ошибка при отправке email: {e}")
        return False

def load_to_sql(properties_data):
    """Загружает данные напрямую в SQL без использования CSV"""
    if not properties_data:
        logger.warning("Нет данных для загрузки в базу данных")
        print("Нет данных для загрузки в базу данных")
        return 0, 0, 1, None
    
    # Подключаемся к базе данных для проверки структуры
    db = DatabaseConnection({k: v for k, v in DB_CONFIG.items() if k != "table"})
    if not db.connect():
        logger.error("ОТЛАДКА: Ошибка подключения к базе данных в load_to_sql")
        print("ОТЛАДКА: Ошибка подключения к базе данных в load_to_sql")
        return 0, 0, 1, None
    
    conn = None
    cursor = None
    
    try:
        # Преобразуем данные в безопасные типы Python
        safe_properties_data = []
        
        for item in properties_data:
            safe_item = {}
            for key, value in item.items():
                if isinstance(value, np.integer):
                    safe_item[key] = int(value)  # numpy.int64 -> int
                elif isinstance(value, np.floating):
                    safe_item[key] = float(value)  # numpy.float64 -> float
                elif isinstance(value, np.bool_):
                    safe_item[key] = bool(value)  # numpy.bool_ -> bool
                else:
                    safe_item[key] = value
            safe_properties_data.append(safe_item)
        
        # Используем новый список с безопасными типами
        properties_data = safe_properties_data
        
        # Преобразуем список словарей в DataFrame для дальнейшей обработки
        df = pd.DataFrame(properties_data)
        
        # Логируем информацию о данных
        logger.info(f"Загрузка {len(df)} записей в базу данных")
        print(f"Загрузка {len(df)} записей в базу данных")
        
        # Очищаем текстовые данные от проблемных символов
        for col in df.columns:
            if df[col].dtype == 'object':  # Для строковых столбцов
                df[col] = df[col].apply(clean_text)
        
        # Удаляем дубликаты по (id, updated_at)
        df = df.drop_duplicates(subset=['id', 'updated_at'])
        logger.info(f"ОТЛАДКА: После удаления дубликатов осталось {len(df)} записей")
        
        # Создаем новое соединение с autocommit=False специально для транзакции
        conn = psycopg2.connect(
            dbname=DB_CONFIG['dbname'],
            user=DB_CONFIG['user'],
            password=DB_CONFIG['password'],
            host=DB_CONFIG['host'],
            port=DB_CONFIG['port']
        )
        conn.autocommit = False  # Устанавливаем autocommit=False для новой транзакции
        cursor = conn.cursor()
        
        try:
            # Сохраняем ID обработанных записей
            processed_ids = []
            
            # Проверяем существование таблицы
            cursor.execute("SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'bayut_properties');")
            table_exists = cursor.fetchone()[0]
            logger.info(f"ОТЛАДКА: Таблица bayut_properties существует: {table_exists}")
            print(f"ОТЛАДКА: Таблица bayut_properties существует: {table_exists}")
            
            if not table_exists:
                logger.error("ОТЛАДКА: Таблица bayut_properties не существует! Необходимо создать таблицу.")
                print("ОТЛАДКА: Таблица bayut_properties не существует! Необходимо создать таблицу.")
                return 0, 0, 1, None
            
            # Получаем список всех колонок
            cursor.execute("""
                SELECT column_name FROM information_schema.columns 
                WHERE table_name = 'bayut_properties' 
                ORDER BY ordinal_position;
            """)
            db_columns = [row[0] for row in cursor.fetchall()]
            logger.info(f"ОТЛАДКА: Колонки в таблице: {db_columns}")
            
            # Создаем список кортежей с данными для вставки
            values = []
            for _, row in df.iterrows():
                property_id = int(row['id']) if pd.notna(row['id']) else None
                title = str(row['title']) if pd.notna(row['title']) else None
                price = float(row['price']) if pd.notna(row['price']) else None
                updated_at = row['updated_at'] if pd.notna(row['updated_at']) else None
                
                values.append((property_id, title, price, updated_at))
                processed_ids.append(property_id)
            
            # SQL запрос для вставки или обновления данных
            sql_query = """
                INSERT INTO bayut_properties 
                (id, title, price, updated_at)
                VALUES %s
                ON CONFLICT (id) DO UPDATE SET
                title = EXCLUDED.title,
                price = EXCLUDED.price,
                updated_at = EXCLUDED.updated_at
            """
            
            logger.info(f"ОТЛАДКА: Выполнение пакетной вставки {len(values)} записей")
            print(f"ОТЛАДКА: Выполнение пакетной вставки {len(values)} записей")
            
            try:
                # Используем execute_values для пакетной вставки
                psycopg2.extras.execute_values(
                    cursor, 
                    sql_query, 
                    values,
                    template=None, 
                    page_size=100
                )
                
                logger.info("ОТЛАДКА: Пакетная вставка успешно выполнена")
                print("ОТЛАДКА: Пакетная вставка успешно выполнена")
                
                # Фиксируем изменения
                conn.commit()
                logger.info("ОТЛАДКА: Изменения зафиксированы успешно")
                print("ОТЛАДКА: Изменения зафиксированы успешно")
                
                # Определяем количество вставленных и обновленных записей
                # Так как мы использовали пакетную вставку, точное количество неизвестно
                new_records = len(values)
                updated_records = 0
                
                logger.info(f"Всего обработано записей: {len(values)}")
                logger.info(f"Добавлено записей: {new_records}")
                
                print(f"Всего обработано записей: {len(values)}")
                print(f"Добавлено записей: {new_records}")
                
                # Ищем максимальную дату обновления
                max_updated_at = None
                if 'updated_at' in df.columns:
                    max_updated_at = df['updated_at'].max()
                    logger.info(f"ОТЛАДКА: Максимальная дата обновления: {max_updated_at}")
                
                return new_records, updated_records, 0, max_updated_at
            
            except Exception as e:
                conn.rollback()
                logger.error(f"ОТЛАДКА: Ошибка при выполнении пакетной вставки: {e}")
                print(f"ОТЛАДКА: Ошибка при выполнении пакетной вставки: {e}")
                raise
        finally:
            cursor.close()
            conn.close()
            logger.info("ОТЛАДКА: Закрыто соединение для транзакции")
            print("ОТЛАДКА: Закрыто соединение для транзакции")
    
    except Exception as e:
        logger.error(f"Ошибка при загрузке данных в SQL: {e}")
        print(f"Ошибка при загрузке данных в SQL: {e}")
        return 0, 0, 1, None

def api_to_sql(max_records=1000, skip_csv=False, send_email=False):
    start_time = datetime.now()
    
    # Создаем таблицу last_run_info, если она не существует
    create_last_run_info_table()
    
    # Получаем дату последнего обновления из базы данных
    last_updated_at = get_last_updated_at()
    logger.info(f"Последняя дата обновления: {last_updated_at}")
    print(f"Последняя дата обновления: {last_updated_at}")
    
    # Устанавливаем параметры API для поиска объектов, обновленных после последней даты
    if last_updated_at:
        formatted_date = last_updated_at.strftime("%Y-%m-%d")
        API_CONFIG["params"]["updatedAt"] = f">={formatted_date}"
        logger.info(f"Запрашиваем объекты, обновленные после {formatted_date}")
        print(f"Запрашиваем объекты, обновленные после {formatted_date}")
    
    # Получаем данные из API
    properties_data, csv_path = fetch_properties(max_records)
    
    # Если получены данные из API
    if properties_data:
        logger.info(f"Получено {len(properties_data)} объектов недвижимости из API")
        print(f"Получено {len(properties_data)} объектов недвижимости из API")
        
        # Обрабатываем полученные данные
        if skip_csv:
            logger.info("Режим прямой загрузки в SQL без CSV")
            print("Режим прямой загрузки в SQL без CSV")
            
            # Загружаем данные напрямую в SQL
            new_records, updated_records, errors, last_updated_sql = load_to_sql(properties_data)
        else:
            logger.info("Режим загрузки данных через CSV")
            print("Режим загрузки данных через CSV")
            
            # Сохраняем данные в CSV
            if csv_path and os.path.exists(csv_path):
                # Загружаем данные из CSV в SQL
                new_records, updated_records, errors, last_updated_sql = csv_to_sql(csv_path)
            else:
                logger.error("CSV файл не найден или не создан")
                print("CSV файл не найден или не создан")
                new_records, updated_records, errors, last_updated_sql = 0, 0, 1, None
        
        # Общая статистика
        logger.info(f"Статистика: добавлено новых - {new_records}, обновлено - {updated_records}, ошибок - {errors}")
        print(f"Статистика: добавлено новых - {new_records}, обновлено - {updated_records}, ошибок - {errors}")
        
        # Обновляем информацию о последнем запуске
        max_updated_at = None
        
        # Ищем максимальную дату обновления в полученных данных
        if properties_data:
            updated_at_values = [prop.get('updated_at') for prop in properties_data if prop.get('updated_at')]
            if updated_at_values:
                max_updated_at = max(updated_at_values)
                logger.info(f"Максимальная дата обновления в полученных данных: {max_updated_at}")
                print(f"Максимальная дата обновления в полученных данных: {max_updated_at}")
        
        # Если не нашли в данных, но есть результат из SQL
        if not max_updated_at and last_updated_sql:
            max_updated_at = last_updated_sql
            logger.info(f"Используем дату обновления из SQL: {max_updated_at}")
            print(f"Используем дату обновления из SQL: {max_updated_at}")
        
        # Обновляем информацию о последнем запуске, если нашли максимальную дату
        if max_updated_at:
            update_last_run_info(max_updated_at)
        
        # Отправляем отчет по электронной почте
        if send_email:
            end_time = datetime.now()
            duration = end_time - start_time
            send_email_report(new_records, updated_records, errors, duration, max_updated_at)
        
        return new_records, updated_records, errors
    else:
        logger.warning("Не получено данных из API")
        print("Не получено данных из API")
        return 0, 0, 1

def main():
    parser = argparse.ArgumentParser(description="Загрузка новых данных из API Bayut в SQL")
    parser.add_argument('--limit', type=int, default=1000, 
                      help='Максимальное количество новых записей для загрузки (по умолчанию 1000)')
    parser.add_argument('--no-csv', action='store_true',
                      help='Не сохранять данные в CSV файл (только в SQL)')
    parser.add_argument('--send-email', action='store_true',
                      help='Отправлять email-отчёт после загрузки (по умолчанию отключено)')
    parser.add_argument('--small', action='store_true',
                      help='Загрузить только 10 записей для тестирования')
    args = parser.parse_args()

    # Если указан флаг --small, устанавливаем лимит в 10 записей
    limit = 10 if args.small else args.limit

    logger.info(f"Запуск скрипта api_to_sql.py с параметрами: лимит={limit}, без CSV={args.no_csv}, отправка email={args.send_email}")
    
    # Создаем таблицу last_run_info, если она не существует
    create_last_run_info_table()
    
    # Логируем начальное количество записей в базе данных
    try:
        logger.info("Проверка начального количества записей в базе данных...")
        with psycopg2.connect(**{k: v for k, v in DB_CONFIG.items() if k != "table"}) as conn:
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM bayut_properties")
                count = cursor.fetchone()[0]
                logger.info(f"Начальное количество записей в таблице bayut_properties: {count}")
                print(f"Начальное количество записей в таблице bayut_properties: {count}")
                
                cursor.execute("SELECT updated_at FROM bayut_properties ORDER BY updated_at DESC LIMIT 1")
                last_update = cursor.fetchone()
                if last_update:
                    logger.info(f"Последняя дата обновления в таблице bayut_properties перед загрузкой: {last_update[0]}")
    except Exception as e:
        logger.error(f"Ошибка при проверке начального количества записей: {e}")
    
    # Загружаем данные из API
    result = api_to_sql(max_records=limit, skip_csv=args.no_csv, send_email=args.send_email)
    
    # Логируем количество записей в базе данных после загрузки
    try:
        logger.info("Проверка количества записей в базе данных после загрузки...")
        with psycopg2.connect(**{k: v for k, v in DB_CONFIG.items() if k != "table"}) as conn:
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM bayut_properties")
                count = cursor.fetchone()[0]
                logger.info(f"Количество записей в таблице bayut_properties после загрузки: {count}")
                print(f"Количество записей в таблице bayut_properties после загрузки: {count}")
                
                cursor.execute("SELECT updated_at FROM bayut_properties ORDER BY updated_at DESC LIMIT 1")
                last_update = cursor.fetchone()
                if last_update:
                    logger.info(f"Последняя дата обновления в таблице bayut_properties: {last_update[0]}")
    except Exception as e:
        logger.error(f"Ошибка при проверке количества записей: {e}")

    return result
def create_last_run_info_table():
    """Создает таблицу last_run_info, если она не существует"""
    try:
        conn = psycopg2.connect(
            dbname=DB_CONFIG['dbname'],
            user=DB_CONFIG['user'],
            password=DB_CONFIG['password'],
            host=DB_CONFIG['host'],
            port=DB_CONFIG['port']
        )
        conn.autocommit = True
        cur = conn.cursor()
        
        # Проверяем существование таблицы
        cur.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_name = 'last_run_info'
            );
        """)
        table_exists = cur.fetchone()[0]
        
        if not table_exists:
            logger.info("ОТЛАДКА: Создание таблицы last_run_info при запуске скрипта")
            print("ОТЛАДКА: Создание таблицы last_run_info при запуске скрипта")
            
            # Создаем таблицу
            cur.execute("""
                CREATE TABLE last_run_info (
                    id SERIAL PRIMARY KEY,
                    script_name VARCHAR(255) NOT NULL,
                    last_run TIMESTAMP,
                    last_updated_date TIMESTAMP,
                    status VARCHAR(50),
                    records_processed INTEGER,
                    UNIQUE(script_name)
                );
            """)
            
            # Проверяем максимальную дату в bayut_properties
            cur.execute("SELECT MAX(updated_at) FROM bayut_properties;")
            max_date = cur.fetchone()[0]
            
            if max_date:
                # Инициализируем запись в last_run_info
                cur.execute("""
                    INSERT INTO last_run_info (script_name, last_run, last_updated_date, status, records_processed)
                    VALUES ('api_to_sql', NOW(), %s, 'INITIALIZED', 0);
                """, (max_date,))
                logger.info(f"ОТЛАДКА: Таблица last_run_info создана и инициализирована с датой {max_date}")
                print(f"ОТЛАДКА: Таблица last_run_info создана и инициализирована с датой {max_date}")
            else:
                # Если в bayut_properties нет данных, инициализируем с датой неделю назад
                week_ago = datetime.now() - timedelta(days=7)
                cur.execute("""
                    INSERT INTO last_run_info (script_name, last_run, last_updated_date, status, records_processed)
                    VALUES ('api_to_sql', NOW(), %s, 'INITIALIZED', 0);
                """, (week_ago,))
                logger.info(f"ОТЛАДКА: Таблица last_run_info создана и инициализирована с датой {week_ago}")
                print(f"ОТЛАДКА: Таблица last_run_info создана и инициализирована с датой {week_ago}")
        else:
            logger.info("ОТЛАДКА: Таблица last_run_info уже существует")
            print("ОТЛАДКА: Таблица last_run_info уже существует")
        
        cur.close()
        conn.close()
        return True
    except Exception as e:
        logger.error(f"ОТЛАДКА: Ошибка при создании таблицы last_run_info: {e}")
        print(f"ОТЛАДКА: Ошибка при создании таблицы last_run_info: {e}")
        return False

if __name__ == "__main__":
    main() 